<?xml version="1.0" encoding="UTF-8" ?>

<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
<channel>
   
      <title>sidhant.io</title>
   
   <link>http://sidhant.io</link>
   <description>Developer Chronicles</description>
   <language>en-us</language>
   <managingEditor> Sidhant Sharma</managingEditor>
   <atom:link href="rss" rel="self" type="application/rss+xml" />
   
	<item>
	  <title>Alfred appreciation post</title>
	  <link>sidhant.io/alfred-appreciation-post</link>
	  <author>Sidhant Sharma</author>
	  <pubDate>2016-05-23T00:00:00+05:30</pubDate>
	  <guid>sidhant.io/alfred-appreciation-post</guid>
	  <description><![CDATA[
	     <p>This post, as the name suggests, is more of an appreciation post. Today, I’m appreciating Alfred. Who’s Alfred, you ask? Well Alfred is my butler!</p>

<p>Nah, not really. I don’t have a butler. But what I do have is nothing short of a butler when it comes to serving files on a network. <a href="https://github.com/tigerkid001/alfred">Alfred</a> is a simple, easy to use command line tool I made sometime ago to, well, share files on a network. ‘Why, oh why’ you ask, when there’s already a million local network sharing applications out there. Two reasons; one, I needed a light local-network file sharing utility that’s simple enough to get going without any configurations and yet good enough to serve multiple files and folders to a number of users. And second, I wanted hands-on with Python, this was a fairly easy task, and a little motivation from someone very awesome, I was convinced I wanted to do this. Now, of course, I could just use Python’s default <a href="https://docs.python.org/2/library/simplehttpserver.html">SimpleHTTPServer</a>, but that plan goes straight out the window when you want to share the files with like a dozen of your friends (taking turns to download isn’t a thing). Ergo, Alfred. Though it was never meant to be anything worth blogging about, I clung to it long enough to make it a useful daily use tool. Alfred can handle multiple connections at once, and over the time, evolved into a pretty nice utility that I practically use everyday. So here’s some stuff Alfred can do:</p>

<p><strong>Serve directories (the basics)</strong></p>

<p>Serve the any directory over the localhost. You can specify a path to the directory you want to served, or you can just skip it and the current one will be served. Default port is <code class="highlighter-rouge">8021</code>, but you override that from the command line.</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nv">$ </span>alfred serve    <span class="c"># Serve current directory</span>
</code></pre></div></div>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nv">$ </span>alfred serve <span class="nt">--path</span> /path/to/directory/   <span class="c"># Serve directory at given path</span>
</code></pre></div></div>

<p>Now you may be wondering why the additional <code class="highlighter-rouge">serve</code> argument. One word, aesthetics.</p>

<p><strong>Serve single files</strong></p>

<p>It often happens that I just want to serve a single file, and not the whole directory. This can be because either I don’t want you to see what’s in that directory (things better left in the dark :expressionless:) or I’m too polite to make you sift through my deeply nested folders or excessive number of files for what you need. So simply,</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nv">$ </span>alfred serve <span class="nt">--path</span> /path/to/file     <span class="c"># Serve a file</span>
</code></pre></div></div>

<p>Yup, the same syntax. Just make it a path to a file and Alfred will serve it individually.</p>

<p><strong>Share links and text</strong></p>

<p>Another cool thing I always find handy is sharing text snippets, code snippets or links. It’s too cumbersome to share such snippets on WhatsApp or Messenger. And Slack won’t work for people not in your team. So, here’s a quick command for it:</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nv">$ </span>alfred serve <span class="nt">--text</span> <span class="s1">'Lorem ipsum dolor set amit.'</span>
</code></pre></div></div>

<p>There’s actually more than just this, so here’s a table of the parameters and what they do:</p>

<table>
  <thead>
    <tr>
      <th>Arguments</th>
      <th>Purpose</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>-h, –help</td>
      <td>show this help message and exit</td>
    </tr>
    <tr>
      <td>-p <em>PORT</em>, –port <em>PORT</em></td>
      <td>specify port to host server on (default: 8021)</td>
    </tr>
    <tr>
      <td>-c <em>COUNT_MAX</em>, –count-max <em>COUNT_MAX</em></td>
      <td>maximum number of times directory can be served (default: 10000)</td>
    </tr>
    <tr>
      <td>-f, –force-port</td>
      <td>force the server to run on specified port (server won’t start if port unavailable)</td>
    </tr>
    <tr>
      <td>-P <em>PATH</em>, –path <em>PATH</em></td>
      <td>specify path (file or directory) to be served (overrides –directory)</td>
    </tr>
    <tr>
      <td>-t <em>TEXT</em>, –text <em>TEXT</em></td>
      <td>specify text to be served (enter text in quotes)</td>
    </tr>
  </tbody>
</table>

<p>And of couse, all the code is available on <a href="https://github.com/tigerkid001/alfred">Github</a> under MIT License (as usual). So feel free to hack as you will.</p>

<p>Yeah, I know what you’re thinking right now. <em>‘Why on EARTH would someone make such a big deal of so little a thing.’</em> Overkill, I get it. But I guess I just keep adding stuff to it that I often need, and accidentally made it this unnecessarily powerful command ine utility that has really no reason to be so. Ah well, typical me. And guessing by the way I tend to overdo things, you can expect some more “features” added to Alfred (I don’t know them yet either, but let’s see :wink:)</p>

	  ]]></description>
	</item>

	<item>
	  <title>The OpenCV Saga</title>
	  <link>sidhant.io/the-opencv-saga</link>
	  <author>Sidhant Sharma</author>
	  <pubDate>2015-10-20T00:00:00+05:30</pubDate>
	  <guid>sidhant.io/the-opencv-saga</guid>
	  <description><![CDATA[
	     <h3 id="ch-1--i-see-you">Ch. 1 : I See You</h3>

<p>College. A temple of knowledge. A place to learn and grow.</p>

<p>Well, not quite. At least not in my experience. Though there isn’t much I’ve gained directly from attending classes (save for the cursory overview to a few subjects), I do admit I have had tonnes of opportunities to learn. And being the ever-curious soul I am, I didn’t miss out out on any.</p>

<p>One such subject that has captured my imagination quite recently is Computer Vision, the magical technology that can make a computer ‘see’.</p>

<p>It started about 2 weeks ago, when my friends and I were brainstorming on what to present as a Minor project. We were running late already (thanks to our first Minor project idea <a href="#minor-one"><sup>1</sup></a>). So, while we were randomly pitching ideas to each other, I suggested making an android application that can print the details of the objects in focus on the screen, in real time. And at this point, we all hopped on to the computer vision train, waving at the stations (aka project ideas) as we passed them. This we did till we reached a project idea that was (seemingly) small, practical and achievable within the given time frame.
The idea was simple. A basic software that will translate your hand gestures into commands for your computer. There was a discussion on what specific type of commands it would support (from general control like navigation through the file system, controlling brightness, volume etc to simply controlling a music application). It was later decided that we will support a rather wild-card entry, Google Chrome browser.
To anyone who follows this blog, Google Chrome would be a surprise. Yes, I know me+Chrome is not a duo you see frequently, but as they say, anything for science.</p>

<p>So, that’s enough backstory. Now on to the good stuff. Being noobs in the field of image processing, we decided to stay with python, for it was more comfortable to most of the team (excluding me, I prefer C/C++. Always.) Anyway, the development started and we did our research,found and discussed some techniques and algorithms.</p>

<p>Fast forward - Coding. The real good stuff.
There are a bunch of OpenCV tutorials available on the Internet, like <a href="http://opencvpython.blogspot.in/">this one</a> and <a href="https://www.pyimagesearch.com/">this one</a>. And of course, never forget to RTFM (OpenCV docs). This gave me a good head start for the task at hand.</p>

<p>I won’t be going into details of the installation process (mainly because it is too boring and I need to catch up on sleep). So, hope you’ll figure it out. (Psst.. unlike me, Google doesn’t sleep)</p>

<h4 id="say-cheese">Say ‘Cheese’!</h4>

<p>First things first. Since our aim is to process gestures in real time, we obviously something to give us some sort of a real-time video input. A modern day contraption that can somehow capture the essence of vision and translate it into a digital stream of bytes. Something so….. screw it, enough Dora-the-explorer. We need a camera. A webcam is the first choice, for it is usually already there on your laptop. It gets tricky when you don’t have a laptop, or a dedicated webcam, as happened with one of our teammates.
Using a webcam with OpenCV is simple, as the following snippet shows:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>
<span class="kn">import</span> <span class="nn">cv2</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="n">np</span>

<span class="c">## Create a capture device instance</span>
<span class="c">## For more:  http://docs.opencv.org/modules/highgui/doc/reading_and_writing_images_and_video.html#videocapture-videocapture</span>
<span class="n">cap</span> <span class="o">=</span> <span class="n">cv2</span><span class="o">.</span><span class="n">VideoCapture</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>

<span class="c">## While the capture device is open</span>
<span class="k">while</span> <span class="n">cap</span><span class="o">.</span><span class="n">isOpened</span><span class="p">():</span>
    <span class="c">## Grab a frame (current image in the memory)</span>
    <span class="n">ret</span><span class="p">,</span> <span class="n">frame</span> <span class="o">=</span> <span class="n">cap</span><span class="o">.</span><span class="n">read</span><span class="p">()</span>
    <span class="c">## Process as desired</span>

<span class="c">## Release camera object when done</span>
<span class="n">cap</span><span class="o">.</span><span class="n">release</span><span class="p">()</span>
</code></pre></div></div>

<p>But not all hope is lost! Turns out, with a little bit of hacking around, it’s possible to use an android device with a camera for the purpose. <a href="https://play.google.com/store/apps/details?id=com.pas.webcam">Here</a> you can find a marvelous little application, <em>IP Webcam</em>, that allows you to locally stream from your mobile camera. You can simply view the stream on the browser/VLC of another device by keying in the IP and the port of the server you run (using the app).
The tricky part is when you need to use it as a video feed and process it in OpenCV. This is where you need to have a work-around. But don’t worry, I saved you the trouble and whipped up this little script which would be useful. Just clone/download <a href="https://github.com/TigerKid001/AndroidCamFeed">this repo</a> or simply get the <code class="highlighter-rouge">[AndroidCamFeed](https://github.com/TigerKid001/AndroidCamFeed/blob/master/AndroidCamFeed.py)</code> file. The usage is simple and demonstrated in the <a href="https://github.com/TigerKid001/AndroidCamFeed/blob/master/AndroidCamFeed.py"><code class="highlighter-rouge">Example.py</code></a>. Import the module and use like you would use the cv2.VideoCapture; the interface is pretty much the same (which, by the way, is no coincidence). Here’s a snippet for example:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c">## Import AndroidCamFeed module</span>
<span class="kn">from</span> <span class="nn">AndroidCamFeed</span> <span class="kn">import</span> <span class="n">AndroidCamFeed</span>

<span class="c">## set host = address of server, in &lt;IP&gt;:&lt;port&gt; format</span>
<span class="c">## Replace the string with your own server's IP and port</span>
<span class="n">host</span> <span class="o">=</span> <span class="s">'192.168.1.2:8080'</span>

<span class="c">## Create new ACF instance with host as parameter</span>
<span class="n">acf</span> <span class="o">=</span> <span class="n">AndroidCamFeed</span><span class="p">(</span><span class="n">host</span><span class="p">)</span>

<span class="c">## While acf is open</span>
<span class="k">while</span> <span class="n">acf</span><span class="o">.</span><span class="n">isOpened</span><span class="p">():</span>

        <span class="c">## Read frame</span>
        <span class="n">frame</span> <span class="o">=</span> <span class="n">acf</span><span class="o">.</span><span class="n">read</span><span class="p">()</span>
        <span class="k">if</span> <span class="n">frame</span> <span class="ow">is</span> <span class="ow">not</span> <span class="bp">None</span><span class="p">:</span>
            <span class="c">## Process frame</span>

<span class="c">## Must release ACF object before exiting.</span>
<span class="n">acf</span><span class="o">.</span><span class="n">release</span><span class="p">()</span>

</code></pre></div></div>

<h4 id="a-sleight-of-hand">A sleight of hand</h4>

<p>The next step we take towards the goal is detecting the hand. This can be tricky and there a many algorithms that try to achieve this. In my personal experience, transforming the image into Y-Cr-Cb color space is effective, especially when coupled with Gaussian Blur and thresholding using Otsu algorithm. Don’t be overwhelmed, we’ll take them all down piece by piece. Starting with color spaces.</p>

<p>The default input from your capture device is in RGB color space. This is the color space we see the world in, with our eyes. Though there are methods to detect skin (and thereby hand) from this color space, they may not be as effective as the others. So we change the color space of the image to the <a href="https://en.wikipedia.org/wiki/YCbCr">Y-Cr-Cb</a> color space. The transformation can be done with literally one line of code:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>
<span class="n">imgYCC</span> <span class="o">=</span> <span class="n">cv2</span><span class="o">.</span><span class="n">cvtColor</span><span class="p">(</span><span class="n">image</span><span class="p">,</span> <span class="n">cv2</span><span class="o">.</span><span class="n">COLOR_BGR2YCR_CB</span><span class="p">)</span>

</code></pre></div></div>

<p>Now that we have the image in the required format, we need to find a way to identify what range of the  Y-Cr-Cb values gives us the clearest view of the skin. Once we have skin, we can figure out the hand.
There are again many options for finding the sweet spot. I’d recommend you do some reading/innovating of your own and find a range that best suits you. For help, you may consider reading through research papers such as <a href="http://arxiv.org/pdf/1212.0134.pdf">this</a>, <a href="http://people.sabanciuniv.edu/mcetin/publications/malima_SIU06.pdf">this</a> and <a href="https://www.google.co.in/webhp?hl=en#hl=en-IN&amp;q=elliptical+boundary+model+skin+color+detection">this</a>. There are plenty more so you can dig as deep as you want.</p>

<p>I experimented both manually (by setting up trackbars and manually setting values to optimal range) and by using the elliptical boundary model for skin color detection. While manual settings offer a better and finer control, it’s a pain to adjust the values.</p>

<p>So, by now you probably have effectively separated the hand component from the rest of the image. Your resulting image may look something like this:</p>

<p><img src="/assets/images/post_images/SkinMask.jpg" alt="SkinMask.jpg" /></p>

<p>A little rough around the edges eh? To soften the rough edges, we can use this simple morphology snippet:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>
<span class="c">## Used for erosion and dilation</span>
<span class="n">kernel</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">((</span><span class="n">size1</span><span class="p">,</span> <span class="n">size2</span><span class="p">),</span> <span class="n">np</span><span class="o">.</span><span class="n">uint8</span><span class="p">)</span>

<span class="c">## Erode then dilate to smoothen out the edges. Check their documentation for more details</span>
<span class="n">frame2</span> <span class="o">=</span> <span class="n">cv2</span><span class="o">.</span><span class="n">erode</span><span class="p">(</span><span class="n">frame</span><span class="p">,</span> <span class="n">kernel</span><span class="p">,</span> <span class="n">iterations</span> <span class="o">=</span> <span class="mi">1</span><span class="p">)</span>
<span class="n">frame2</span> <span class="o">=</span> <span class="n">cv2</span><span class="o">.</span><span class="n">dilate</span><span class="p">(</span><span class="n">frame</span><span class="p">,</span> <span class="n">kernel</span><span class="p">,</span> <span class="n">iterations</span> <span class="o">=</span> <span class="mi">1</span><span class="p">)</span>

<span class="c">## Use opening and closing morphological transformations to remove those annoying little dots</span>
<span class="n">frame2</span> <span class="o">=</span> <span class="n">cv2</span><span class="o">.</span><span class="n">morphologyEx</span><span class="p">(</span><span class="n">frame</span><span class="p">,</span> <span class="n">cv2</span><span class="o">.</span><span class="n">MORPH_OPEN</span><span class="p">,</span> <span class="n">kernel</span><span class="p">)</span>
<span class="n">frame2</span> <span class="o">=</span> <span class="n">cv2</span><span class="o">.</span><span class="n">morphologyEx</span><span class="p">(</span><span class="n">frame</span><span class="p">,</span> <span class="n">cv2</span><span class="o">.</span><span class="n">MORPH_CLOSE</span><span class="p">,</span> <span class="n">kernel</span><span class="p">)</span>

</code></pre></div></div>

<p>To smoothen it out more, we use <a href="https://en.wikipedia.org/wiki/Gaussian_blur">Gaussian Blur</a> and then <a href="http://docs.opencv.org/modules/imgproc/doc/miscellaneous_transformations.html">threshold</a> using the <a href="https://en.wikipedia.org/wiki/Otsu's_method">Otsu algorithm</a> as shown:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>
<span class="c">## Here, (x, y) is a tuple denoting the Gaussian kernel size, with Gaussian kernel standard deviation ## in X direction being 0, (for Y direction, it is 0 by default)</span>
<span class="n">imgYCC</span> <span class="o">=</span> <span class="n">cv2</span><span class="o">.</span><span class="n">GaussianBlur</span><span class="p">(</span><span class="n">imgYCC</span><span class="p">,</span> <span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">),</span> <span class="mi">0</span><span class="p">)</span>

<span class="c">## Set 0 and 255 to the desired minimum and maximum thresholds. (Psst.. experiment time)</span>
<span class="n">ret</span><span class="p">,</span> <span class="n">imgYCC</span> <span class="o">=</span> <span class="n">cv2</span><span class="o">.</span><span class="n">threshold</span><span class="p">(</span><span class="n">imgYCC</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">255</span><span class="p">,</span> <span class="n">cv2</span><span class="o">.</span><span class="n">THRESH_OTSU</span><span class="p">)</span>

</code></pre></div></div>
<p><em>yawnnn…</em> OK, so now we have a nice and smooth image to work on. Looking back at it all, it doesn’t look like we have a long way, but believe me, getting here the way we did is big task. Extracting the hand out of an RGB (or BGR, as it used in OpenCV) image is half the job (well, a little less than half, but you get the point). In the next post, we’ll discuss a little more about hand exatraction, specifically blob analysis. Then we’ll continue on to detecting the hand, and experiment with algorithms to accurately detect parts of the hand.</p>

<p>And this is the point where I am looking at you, no expression, a blank poker face, hoping you’ll get the signal, say thanks and bid farewell.. No, I don’t mean you to go, it’s just.. I’m tired and there’s a lot more I’d like to talk about in more detail, but some other time. (Very soon, I promise). So, there you go. You can go on and experiment, research, play/mess with that code.
And if you like, you can even check out our project <a href="http://github.com/TigerKid001/Dex">Dex</a>, as we call it. Help will be much appreciated ;)</p>

<p>As a great wizard once said,</p>

<blockquote>
  <p>Fly, you fools!</p>
</blockquote>

<p>Happy coding :)
<br /><br /></p>

<hr />

<p><strong><sup id="minor-one">1</sup></strong>
The first Minor project idea was that of an automated bot that would crack Google’s new NoCaptcha reCaptcha system by imitating organic mouse movements and selecting the correct images for the given keyword. Sadly, it was dropped because of time constraints.</p>

	  ]]></description>
	</item>


</channel>
</rss>
